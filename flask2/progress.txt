November 18
Two unintuitive results from today
    - Normalizing of images was done originally by dividing by 255 and subtracting 0.5. I changed it to divide by the max of the training images, and the error got worse! the audacity! 2.72 -> 2.83 cm on calib dataset, and 1.817 -> 2.0 cm on the val dataset. Intuitively, I thought this would remove some error from the bounds being off, but it seems I was wrong. Weird.
    - Also, apparently greyscaling the images improves performance. The dataset is only a few hundred images of eyes, but that's SO WEIRD! Maybe the model just learned a bunch of noise from the gamma boosting. hmm. Gonna try to gamma boost the testing for the regression model.


November 13
Model + calibration working acceptably. Pretty consistently 3x3 is accurate. I'm speccing just how accurate it is, but eval is throwing the "please wait for loadeddata" event in video. I forgot how I fixed this a while back, I think it was messing with the order of the includes but I don't remember exactly. GOT IT! Basically, put svr.js import before the collectioncolor.js. I think cause they both have a setup camera function. Unsure, because they're the same one. Whatever. Works now.

November 2
Decided to just do training over the embeddings. Weird error related to webgl that I cannot fix on my own when we try to train on the base model. So we're feeding in the 8, 4, and 2 vecs from the last dense stack of the net, then the 8 eye corner coordinates, and then the head yaw pitch roll and area within the image. Seems to work pretty ok if given enough iterations and data. 26-vec of data, maybe 100 iterations. Very shallow model follows it.
Need to write the data collection page in a coherent way, but I guess that comes after we get the thing actually working.

TODO TODAY:
- Add error calculation (x and y separately) in pixels and cm to the eval loop of the sketch.js. Also add a reset button
- Try some data collection schemes
    - head still, one round of 10 im/loc
    - head rotate, one round of 10 im/loc
    - head still and rotate, one round each of 10 im/loc



October 28
Made video resolution 1280x1280 at most, any larger adds too much overhead without increasing pixel capture density

To freeze layers, iterate through the model.layers Array and set their .trainable = false.

To get embeddings, pop off layers and generate a new model and use that. Not sure what to do with graph models
const model = tf.sequential({
    layers: baseModel.layers
  });
graph models use the tf.model({}) function, which doesn't take layers but does take inputs and outputs. Can't give a layer directly (a tensor), but need to give the .input or .output variable.
var modelcopy = tf.model({inputs: naturemodel.inputs, outputs: naturemodel.layers[29].output}); // Outputs 8 length vec
var modelcopy = tf.model({inputs: naturemodel.inputs, outputs: naturemodel.layers[28].output}); // Outputs 4112 length vec from the concat
Credits: https://stackoverflow.com/questions/50942677/tensorflowjs-how-to-get-inner-layer-output-in-a-cnn-prediction

Making copy of the model using the above trick does not fix the NaN during inference but not during loss thing. Wack!

Aug 13
Added higher resolution capture, slows down facemesh continuous execution considerably. I can capture two video resolutions however, and feed one into facemesh to scale up to get higher resolution eye pictures. Pretty cool actually.
Added copying of the face mesh points to the data capture. Need to incorporate into the training.

TODO:
try to convert the Caffe iTracker models into TF, into TF.js
Add face mesh points to the classifier features and training
channel video2 into face mesh, then scale up the points for the capture.


Aug 11
Changed faceSize function because [x,y] = function() isn't acceptable in js anymore or something.
Switched to classification of the 9 points, didn't work better. Also noticed there's a point missing from the calibration dataset. Going to add back later, since I know it's going to screw some stuff up that relies on that index.
Added variable to switch from classifier to regressor
Doesn't work on phone for some reason. Gotta test and compare with regression later.


Aug 9
Changed model to use mobilenet embeddings (v1 has embeddings of len 1024, toggleable with .infer(im, embedding=true)). Performance seems about equivalent.



Aug 8
WASM breakthrough, turns out shallow neural network training works but anything with hidden layers is OUT. Such a silly bug... Now that it's fixed, we can move on to improving existing model.
Need to up calib rounds and standardize num pics taken at each point. Should take a bit of juggling.

TODO:
download gazecapture dataset
train a model on that dataset. Probably need to use desktop for that.


July 28 night
Added alternative regression library to use at the end of training on the calibration page. Seems to work ok, however the memory leak is back in full force. Lags the hell out of the webcam, consumes a bunch of memory. Unsure where the problem is coming from, will review code tomorrow



July 27
Mobile has a new problem: my phone restarts whenever it finishes collecting data >:(. Unsure if it's just because my phone is a bit old. it does run a little laggy on my phone...
Fixed memory leak in /transfer. It was a combination of tensors being pushed and not tossed, and tf tidys that weren't properly put around everything.
Also fixed memory leak in the loaded model


July 23
Patched the calib rounds and delays to be more friendly
Still doesn't work on the phone, something about wasm being uninitialized and webgl unable to be loaded. Poison bug I think.
Memory leak also present in the calibration page :(. Patched a bit with tf.tidy, but seems to be somewhere in the predictions page. Unsure where it's coming from, can look at it more later.

TODO:
Need to rewrite this page into a more manageable code structure.
Pretty up the page so user won't have to see themselves all the time.
Add Accel


July 22
Migrated Karan's calib file, integrated the data collection and training.
Got output prediction working on desktop as a dot.
Ran into issues on mobile browser, something about the backends. Gonna have to debug that later.
found a memory leak in live test, makes the thing go to garbage in like 4 seconds. wild.

TODO:
debug mobile browser transfer learning page



July 21
Added colored eyeData save
Added an eyeData with color photos to the transfer page
Ran a small vector through the mobilenet embeddings and trained
Began porting over real-time capture code from gaze_calib

TODO:
Get vector of eyes from the calib dots, instead of using dotgenerator
change backends and train the mobilenet output model
have it output live prediction as a dot or something


July 17
Reduced the model size for the two input eyes and the boosted model, maintaining accuracy.
Created the /test/ page and all backend code for evaluating the model.

TODO next time:
Make sense of the accel
smooth the eye predictions
speed up eye predictions
increase accuracy of eye prediction
add calibration to testing page


July 15
Collected a lot of data, trained some bad models. Final validation error was coming out like 20%.
Changed the eye model architecture, I'm down to 11% validation error.
Training a boosted model with the two eye predictions, head yaw and pitch, and the head distance, the whole thing is 5% error in terms of the screen height and width. This is added error, and my phone screen is around 5x10 cm, meaning we're sub-centimeter when it comes to error. Not bad.
Saved the boosted model. Need to make a separate page to test out the model live.

TODO next time:
deploy live testing of the model. It probably won't work on desktop, since it's a mobile model.
Visualize accel, or at least figure out how to use it to interpret gestures.


July 14
Added head distance, scaled as a proportion of the head to the view of the screen.
Training a left eye and right eye model, then boosting that output with a simple 5-50-2 NN. Features are the two outputs, head tilts and distance. Should be simple, just gotta set it up.



July 12 - night
made the separate page for transfer learning and started on the tutorial. Mobilenet takes 224x224x3 images.
Figured out mobilenet takes color images. Also I don't think an object classifier is going to do much for us once we transfer learning it.


TODO next time:
Collect ~2k eye pictures and get a usable model trained
visualize accelerometer in 3 axes


July 12 - morning
Got the eye capture working on the phone by using WASM backend, webgl doesn't work for whatever reason.
Hosting the Flask-Frozen site on netlify. Very convenient, offers HTTPS and phone access
iOS Safari Accel access is granted by the button, they made it very difficult for browsers to capture that without the user knowing so it must be manually triggered.

TODO next time:
transfer learning basic implementation from tutorial (https://codelabs.developers.google.com/codelabs/tensorflowjs-teachablemachine-codelab/index.html#6)
take parts of the transfer learning that you need, probably using just FC layers? Input the two eye activations, as well as head pitch and yaw, output is eyeVals.
Make a separate page for the transfer learning as well as mobile net training to keep it neat.
If mobilenet + some data is good enough, save that model. Otherwise, train a usable one.
Visualize the accel in 3 axes


July 11
Ported over eye-grabbing code and facemesh implementation. Saving data works now, as does importing it.

TODO next time:
get it working on the phone