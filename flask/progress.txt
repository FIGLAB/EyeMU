
July 27
Mobile has a new problem: my phone restarts whenever it finishes collecting data >:(. Unsure if it's just because my phone is a bit old. it does run a little laggy on my phone...
Fixed memory leak in /transfer. It was a combination of tensors being pushed and not tossed, and tf tidys that weren't properly put around everything.
Also fixed memory leak in the loaded model


July 23
Patched the calib rounds and delays to be more friendly
Still doesn't work on the phone, something about wasm being uninitialized and webgl unable to be loaded. Poison bug I think.
Memory leak also present in the calibration page :(. Patched a bit with tf.tidy, but seems to be somewhere in the predictions page. Unsure where it's coming from, can look at it more later.

TODO:
Need to rewrite this page into a more manageable code structure.
Pretty up the page so user won't have to see themselves all the time.
Add Accel


July 22
Migrated Karan's calib file, integrated the data collection and training.
Got output prediction working on desktop as a dot.
Ran into issues on mobile browser, something about the backends. Gonna have to debug that later.
found a memory leak in live test, makes the thing go to garbage in like 4 seconds. wild.

TODO:
debug mobile browser transfer learning page



July 21
Added colored eyeData save
Added an eyeData with color photos to the transfer page
Ran a small vector through the mobilenet embeddings and trained
Began porting over real-time capture code from gaze_calib

TODO:
Get vector of eyes from the calib dots, instead of using dotgenerator
change backends and train the mobilenet output model
have it output live prediction as a dot or something


July 17
Reduced the model size for the two input eyes and the boosted model, maintaining accuracy.
Created the /test/ page and all backend code for evaluating the model.

TODO next time:
Make sense of the accel
smooth the eye predictions
speed up eye predictions
increase accuracy of eye prediction
add calibration to testing page


July 15
Collected a lot of data, trained some bad models. Final validation error was coming out like 20%.
Changed the eye model architecture, I'm down to 11% validation error.
Training a boosted model with the two eye predictions, head yaw and pitch, and the head distance, the whole thing is 5% error in terms of the screen height and width. This is added error, and my phone screen is around 5x10 cm, meaning we're sub-centimeter when it comes to error. Not bad.
Saved the boosted model. Need to make a separate page to test out the model live.

TODO next time:
deploy live testing of the model. It probably won't work on desktop, since it's a mobile model.
Visualize accel, or at least figure out how to use it to interpret gestures.


July 14
Added head distance, scaled as a proportion of the head to the view of the screen.
Training a left eye and right eye model, then boosting that output with a simple 5-50-2 NN. Features are the two outputs, head tilts and distance. Should be simple, just gotta set it up.



July 12 - night
made the separate page for transfer learning and started on the tutorial. Mobilenet takes 224x224x3 images.
Figured out mobilenet takes color images. Also I don't think an object classifier is going to do much for us once we transfer learning it.


TODO next time:
Collect ~2k eye pictures and get a usable model trained
visualize accelerometer in 3 axes


July 12 - morning
Got the eye capture working on the phone by using WASM backend, webgl doesn't work for whatever reason.
Hosting the Flask-Frozen site on netlify. Very convenient, offers HTTPS and phone access
iOS Safari Accel access is granted by the button, they made it very difficult for browsers to capture that without the user knowing so it must be manually triggered.

TODO next time:
transfer learning basic implementation from tutorial (https://codelabs.developers.google.com/codelabs/tensorflowjs-teachablemachine-codelab/index.html#6)
take parts of the transfer learning that you need, probably using just FC layers? Input the two eye activations, as well as head pitch and yaw, output is eyeVals.
Make a separate page for the transfer learning as well as mobile net training to keep it neat.
If mobilenet + some data is good enough, save that model. Otherwise, train a usable one.
Visualize the accel in 3 axes


July 11
Ported over eye-grabbing code and facemesh implementation. Saving data works now, as does importing it.

TODO next time:
get it working on the phone